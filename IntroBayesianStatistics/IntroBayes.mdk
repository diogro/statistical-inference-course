[INCLUDE=present.mdk]
Title         : Probabilistic Modeling
Sub Title     : Introduction to Bayesian Statistics
Author        : Diogo Melo
Affiliation   : Lewis-Sigler Institute of Integrative Genomics
Email         : damelo@princeton.edu
Reveal Theme  : solarized
Beamer Theme  : singapore
Package       : pstricks
Package       : pst-plot
Bib           : ../shared/references.bib
Csl Style     : ../shared/evolution.csl
Cite Style    : natural
@html .math-inline, .math-display: color=#586e75

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG-full" type="text/javascript"></script>

[TITLE]

# Intro 

## Maximum Likelihood estimation

  - Up to this point we have been using the Maximum Likelihood principle
  - ML has many advantages:

    - General
    - (mostly) Automatic
    - Good theoretical justification
    - Good performance

## The Maximum Likelihood way

   Given some data...
  ~~ Math
  y = \{ y_1, \cdots, y_n \}
  ~~
  Define an observational model: 
  ~~ Math
  P(y|\theta) = L_y(\theta)
  ~~
  Maximize the likelihood over the parameter space: 
  ~~ Math
  \hat{\theta} =\underset{\theta \in \Omega}{\operatorname{arg max}} \left [ L_y(\theta) \right ]
  ~~
  Any further inference uses this ML estimator

  ~ Center
  ~~ Math 
  \mu = f(\hat{\theta}) 
  ~~ 
  ~

## Log Likelihood surface


~~ Center

![surface]

~ Math
y \sim Normal(\alpha + \beta x, \sigma)
~
~~

[surface]: ../figures/logliksurface.png "A log likelihood surface" { height: 500px }

# Bayes

# rev thomas bayes (1701 – 1761)

~~ Center
![](../figures/thomas_bayes.gif){height: 500px}
~~

# Product rule (or Bayes Theorem)

~ Center
~~ Math
P(AB) = P(A|B)P(B) = P(B|A)P(A)
~~

~

## What is Bayesian Statistics?

We can think of Bayesian Statistics as an extension of ML

- ML: What is the parameter value that maximizes the probability of having generated the data:

  ~ Center
  ~~ Math { padding:.3em; \
    background-color:floralwhite; \
    background-clip: padding-box; \
    border: 0.3ex solid black; \
    border-radius: 10ex; \
    width: 9em; \
    vertical-align: middle; }
  \underset{\theta \in \Omega}{\operatorname{arg max}} \left [ P(y|\theta) \right ]
  ~~
  ~

- Bayes: What is the probability distribution of parameter values given the data:
  ~ Center
  ~~ Math { padding:.3em; \
    background-color:floralwhite; \
    background-clip: padding-box; \
    border: 0.3ex solid black; \
    border-radius: 10ex; \
    width: 10em; \
    vertical-align: middle; }
  P(\theta|y) \propto P(\theta)P(y|\theta)
  ~~
  ~

## Probability Theory: The Logic of Science [-@jaynes2003]

~ Center
![](../figures/jaynes_book.jpeg){height: 500px}
~ 

By Edwin Thompson Jaynes (1922 – 1998) 

## Statistical rethinking [-@rethinking]

~ Center
![](../figures/rethinking_book.jpg){height: 500px}
~ 

By Richard McElreath ([youtube.com/\@rmcelreath](https://www.youtube.com/@rmcelreath))

## Prior $\times$ Likelihood $\propto$ Posterior

~ Center
![](../figures/prior_likelihood.png){height: 600px}
~

## why use the posterior?

- Allows us to use probability in more contexts
- {.fragment} $P(\theta|y)$ represents our knowledge of parameters using probability 
  - this representation fully encapsulates our beliefs
- {.fragment} $P(\theta)$, the prior, can encode useful information
    - parameter scale, shared structure, permitted values...
- {.fragment} Isn't the MLE the best estimator? (depends on the criteria...)
  - Sometimes... but not $\mu = f(\hat{\theta})$ 
- {.fragment} Expands the range of models we can fit

## confidence intervals

~~ Center

![intervals]

~~

[intervals]: ../figures/confidence_intervals.png "Posterior confidence intervals" { height: 500px }


## nuisance variables

We can add additional variables that help to define the model but are not present in the posterior

~~ Math
P(\theta|y) \propto \int P(\theta) P(y|\theta, \nu) d\nu
~~ 


# Using the posterior

## Posterior estimators 

Bayesian equivalent to MLE is the **M**aximum **A** **P**osteriori (MAP):

~~ Math
\hat \theta^{MAP}  = \underset{\theta \in \Omega}{\operatorname{arg max}} \left [ P(\theta|y) \right ]
~~

The posterior mean is more common:

~~ Math
E_y[\theta]  = \sum_{\theta \in \Omega} \theta P(\theta|y) 
~~

(Posterior median is also used occasionally.) 

## Posterior approximations

- For a small number of models we can write the posterior distribution directly (really small, don't bother).

- For most models, we use posterior samples to approximate the posterior.

~~ Math
\{\theta_1, \cdots, \theta_N\} \sim P(\theta|y)
~~ 

## Posterior sample histogram

~ Center
![Posterior samples and posterior density](../figures/posterior_samples.png){height: 500px}
~

## Posterior derived QUANTITIES 

This sample can be used to calculate any quantity of interest
~~ Math
    \{\theta_1, \cdots, \theta_N\} \sim P(\theta|y)
~~ 
For example, the posterior mean is just:
~~ Math
     \frac{\theta_1 + \theta_2 + \cdots + \theta_N}{N} \approx \sum_{\theta \in \Omega} \theta P(\theta|y) 
~~ 

## Other quantities

- Any other functions of the parameters can be estimated from the samples
- A common use is to calculate contrast between categorical levels, estimating the difference between groups

## Categorial predictors

We can also use categorical predictors to estimate per-group averages. 

- $K_i$: caloric content of milk in several monkey groups
- $CLADE$: categorical variable for the monkey groups

~~ Math
\begin{aligned}
  K_i &\sim Normal(\mu_i, \sigma) \\
  \mu_i &= \alpha_{CLADE[i]} \\
  \alpha_i &\sim Normal(0, 0.5) \\
  \sigma &\sim Exponential(1)
\end{aligned}
~~

## Per clade milk content

~ Center
![](../figures/milk_model_coef.png)
~

## contrasts

To compare coefficient estimates we must look at the distribution of differences

~ Center
![](../figures/milk_model_constrast.png)
~

# Building a model

## My first bayesian regression model

Given the matched pairs:
~~ Math
(x, y) = \{ (x_1, y_1), \cdots, (x_n, y_n) \}
~~
Define a likelihood:
~~ Math
y_i \sim Normal(\mu_i, \sigma)
~~
~~ Math
\mu_i = \alpha + \beta x_i 
~~
And a set of priors on the parameters:
~~Math
\alpha \sim P(\alpha)
~~
~~Math
\beta \sim P(\beta)
~~
~~Math
\sigma \sim P(\sigma)
~~

## How do we choose the priors?

- Agnostic choices
  - Laplace and the Principle of indifference
  - "Uninformative" priors
- {.fragment} Maximum entropy priors
  - priors that encode the least amount of information given constraints
- {.fragment} Jeffreys priors
  - invariant under a change of coordinates
- {.fragment} Hard constraints
  - restricted domains (e.g. variance must be positive)

## Good prior choices

- Use domain expertise!
  - Knowledge of scale (height $\sim$ weight example)
  - Experimental design (more in the hierarchical models class)
  - Using simulations to understand the implications of priors

## priors can be used to encode scale information

~~ Center

![weight_height]

~~

Adapted from @rethinking.

[weight_height]: ../figures/height_weight_regression.png "height weight from data(Howell1)" { height: 500px }


## Wide prior 

~~ Center

![wide]

~~

This is sometimes called a non-informative prior

[wide]: ../figures/wide_prior.png "height weight from data(Howell1)" { height: 500px }

## sensible priors

~ Center
![sensible]
~

This prior is informative, but in a good way!

[sensible]: ../figures/sensible_prior.png "height weight from data(Howell1)" { height: 500px }

## Our model for the height data

~ Begin Columns

~ Column 

![weight_height]

~

~ Column 

~~ Math
\begin{aligned} 
  y_i &\sim Normal(\mu_i, \sigma) \\
  \mu_i &= \alpha + \beta x_i  \\
  \alpha &\sim Normal(0, 20) \\
  \beta &\sim lognormal(0, 1) \\
  \sigma &\sim Exponential(1) \\
\end{aligned} 
~~

~

~ End Columns

## Posterior samples

```python
> samples
# A tibble: 2,000 × 3
           a         b     sigma
   <dbl[1d]> <dbl[1d]> <dbl[1d]>
 1      115.     0.889      4.78
 2      109.     1.02       5.30
 3      112.     0.928      5.07
 4      111.     0.949      5.30
 5      111.     0.955      5.04
 6      115.     0.872      5.19
 7      109.     1.01       5.13
 8      117.     0.844      5.00
 9      115.     0.882      4.94
10      112.     0.939      4.95
# … with 1,990 more rows
```

## Posterior mean estimates

```
> colMeans(samples)
          a           b       sigma 
112.9296580   0.9253803   5.0453651 
```

## Model fit

~ Center
![](../figures/height_weight_fit.png){height: 500px}
~

## Posterior parameter distribution

~ Center
![](../figures/height_weight_posteriors.png){height: 500px}
~

# References

[BIB]