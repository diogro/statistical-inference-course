[INCLUDE=present.mdk]
Title         : Probabilistic Modeling
Sub Title     : Introduction to Bayesian Statistics
Author        : Diogo Melo
Affiliation   : Lewis-Sigler Institute of Intergrative Genomics
Email         : damelo@princeton.edu
Reveal Theme  : solarized
Beamer Theme  : singapore
Package       : pstricks
Package       : pst-plot
Bib           : ../shared/references.bib
Csl Style     : ../shared/evolution.csl
Cite Style    : natural
@html .math-inline, .math-display: color=#586e75

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG-full" type="text/javascript"></script>

[TITLE]

# Intro 

## Maximum Likelihood estimation

  - Up to this point we have been using the Maximum Likelihood principle
  - ML has many advantages:

    - General
    - (mostly) Automatic
    - Good theoretical justification
    - Good performance

## The Maximum Likelihood way

   Given some data...
  ~~ Math
  y = \{ y_1, \cdots, y_n \}
  ~~
  Define an observational model: 
  ~~ Math
  P(y|\theta) = L_y(\theta)
  ~~
  Maximize the likelihood over the parameter space: 
  ~~ Math
  \hat{\theta} =\underset{\theta \in \Omega}{\operatorname{arg max}} \left [ L_y(\theta) \right ]
  ~~
  Any further inference uses this ML estimator

  ~ Center
  ~~ Math 
  \mu = f(\hat{\theta}) 
  ~~ 
  ~

## Log Likelihood surface


~~ Center

![surface]

~ Math
y \sim Normal(\mu + \beta x, \sigma)
~
~~

[surface]: ../figures/logliksurface.png "A log likelihood surface" { height: 500px }

# Bayes

# rev thomas bayes (1701 – 1761)

~~ Center
![](../figures/thomas_bayes.gif){height: 500px}
~~

# Product rule (or Bayes Theorem)

~ Center
~~ Math
P(AB) = P(A|B)P(B) = P(B|A)P(A)
~~

~

## What is Bayesian Statistics?

We can think of Bayesian Statistics as an extension of ML

- ML: What is the parameter value with the highest probability of having generated the data:

  ~ Center
  ~~ Math { padding:.3em; \
    background-color:floralwhite; \
    background-clip: padding-box; \
    border: 0.3ex solid black; \
    border-radius: 10ex; \
    width: 9em; \
    vertical-align: middle; }
  \underset{\theta \in \Omega}{\operatorname{arg max}} \left [ P(y|\theta) \right ]
  ~~
  ~

- Bayes: What the distribution of parameter values given the data:
  ~ Center
  ~~ Math { padding:.3em; \
    background-color:floralwhite; \
    background-clip: padding-box; \
    border: 0.3ex solid black; \
    border-radius: 10ex; \
    width: 10em; \
    vertical-align: middle; }
  P(\theta|y) \propto P(\theta)P(y|\theta)
  ~~
  ~

## Probability Theory: The Logic of Science (2003)

~ Center
![](../figures/jaynes_book.jpeg){height: 500px}
~ 
By Edwin Thompson Jaynes (1922 – 1998) 


## Prior $\times$ Likelihood $\propto$ Posterior

~ Center
![](../figures/prior_likelihood.png){height: 600px}
~

## why use the posterior?

Isn't the MAP the best estimator? (depends on the criteria...)

- $P(\theta|y)$ represents our knowledge of parameters using probability 
  - this representation fully encapsulates our beliefs
- $P(\theta)$, the prior, can encode useful information
    - parameter scale, shared structure, permited values...
- Expands the range of models we can fit
- Allows us to use probability in more contexts

## confidence intervals

~~ Center

![intervals]

~~

[intervals]: ../figures/confidence_intervals.png "Posterior confidence intervals" { height: 500px }


## nuisance variables

We can add additional variables that help to define the model but are not present in the posterior

~~ Math
P(\theta|y) \propto \int P(\theta) P(y|\theta, \nu) d\nu
~~ 


# Using the posterior

## Posterior estimators 

Bayesian equivalent to MLE is the **M**aximum **A** **P**osteriori (MAP):

~~ Math
\hat \theta^{MAP}  = \underset{\theta \in \Omega}{\operatorname{arg max}} \left [ P(\theta|y) \right ]
~~

The posterior mean is more common:

~~ Math
E_y[\theta]  = \sum_{\theta \in \Omega} \theta P(\theta|y) 
~~

(Posterior median is also used occasionally.) 

## Posterior approximations

- For a small number of models we can write the posterior distribution directly (really small, don't bother).

- For most models, we use posterior samples to approximate the posterior.

~~ Math
\{\theta_1, \cdots, \theta_N\} \sim P(\theta|y)
~~ 

## Posterior sample histogram

~ Center
![Posterior samples and posterior density](../figures/posterior_samples.png){height: 500px}
~

## Posterior derived QUANTITIES 

This sample can be used to calculate any quantity of interest
~~ Math
    \{\theta_1, \cdots, \theta_N\} \sim P(\theta|y)
~~ 
For example, the posterior mean is just:
~~ Math
     \frac{\theta_1 + \theta_2 + \cdots + \theta_N}{N} \approx \sum_{\theta \in \Omega} \theta P(\theta|y) 
~~ 

## Other quantities

- Any other functions of the parameters can be estimated from the samples
- The most common use is to calculate contrasts

## Constrasts

Figure of contrasts

# Building a model

## My first bayesian regression model

Given the matched pairs:
~~ Math
(x, y) = \{ (x_1, y_1), \cdots, (x_n, y_n) \}
~~
Define a likelihood:
~~ Math
y_i \sim Normal(\mu_i, \sigma)
~~
~~ Math
\mu_i = \alpha + \beta x_i 
~~
And a set of priors on the parameters:
~~Math
\alpha \sim P(a)
~~
~~Math
\beta \sim P(b)
~~
~~Math
\sigma \sim P(\sigma)
~~

## How do we choose the priors?

- Using domain expertise 
  - Knowledge of scale (height $\sim$ weight example)
  - Experimental design (more in the hierarchical models class)
  - Using simulations to understand the implications of priors
- Hard constraints
  - restricted domains (e.g. variance must be positive)


## priors can be used to encode scale information

~~ Center

![weight_height]

~~

Adapted from [@rethinking].

[weight_height]: ../figures/height_weight_regression.png "height weight from data(Howell1)" { height: 500px }


## Wide prior 

~~ Center

![wide]

~~

This is sometimes refered to as a non-informative prior

[wide]: ../figures/wide_prior.png "height weight from data(Howell1)" { height: 500px }

## sensible priors

~~ Center

![sensible]

~~

This prior is informative, but in a good way!

[sensible]: ../figures/sensible_prior.png "height weight from data(Howell1)" { height: 500px }