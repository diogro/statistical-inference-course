[INCLUDE=present.mdk]
Title         : Bayesian model fitting
Sub Title     : MCMC sampling
Author        : Diogo Melo
Affiliation   : Lewis-Sigler Institute of Integrative Genomics
Email         : damelo@princeton.edu
Reveal Theme  : solarized
Beamer Theme  : singapore
Package       : pstricks
Package       : pst-plot
@html .math-inline, .math-display: color=#586e75

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG-full" type="text/javascript"></script>

[TITLE]

## Model fit

~ Center
![](../figures/height_weight_fit.png){height: 500px}
~

## Posterior samples

```python
> samples
# A tibble: 2,000 × 3
           a         b     sigma
   <dbl[1d]> <dbl[1d]> <dbl[1d]>
 1      115.     0.889      4.78
 2      109.     1.02       5.30
 3      112.     0.928      5.07
 4      111.     0.949      5.30
 5      111.     0.955      5.04
 6      115.     0.872      5.19
 7      109.     1.01       5.13
 8      117.     0.844      5.00
 9      115.     0.882      4.94
10      112.     0.939      4.95
# … with 1,990 more rows
```

# How to get posterior samples?

- There is no general method to find high probability regions in arbitrary probability distributions.

- This mean most models are fit using purely computational methods.

## curse of dimensionality 

- For simple parameters spaces, we can do grid search or some brute force method to find high probability regions

- This breaks down quickly as the number of parameters increases

## high dimensional watermelons are awful 

![](../figures/watermelon.svg)


## typical set 

~ Center
![](../figures/typical_set.svg){ width: 500px }
~

## Multivariate gaussians

~ Center
![](../figures/MultivariateNormal.png){ width: 500px }
~

## Distance from mean in 2 dimensions

~ Center
![](../figures/2d_gauss.png){ width: 500px }
~

## Gaussian typical set

~ Center
![](../figures/typical_set.gif)
~

## finding high probability regions

~ Center
![](../figures/typical_set2.png){ width: 600px }
~

## mcmc method

~ Center
![](../figures/MCMC_typical.png){ height: 600px }
~


## MCMC samplers 

- {.fragment} Metropolis–Hastings algorithms (broad class of samplers, very general)
  - Most methods in the wild are some flavor of this 
- {.fragment} Reversible Jump MCMC (used in many phylogenetic packages)
  - Allows for posterior distributions with variable dimensionality
- {.fragment} Usable non-mcmc methods: R-INLA - integrated nested Laplace approximation.
  - Great for structural equation modeling, much faster for some classes of models

## MCMC samplers 2

- {.fragment} Gibbs samplers
    - Mostly surpassed, but still in wide use.
    - Can sample discrete parameters.
    - Requires particular types of priors.
    - Software: WinBugs, Bugs, Jags...
- {.fragment} Hamiltonian Monte Carlo samplers
    - Latest generations samplers.
    - Fast and general.
    - Discrete parameters must be integrated.
    - Can fit dynamic models using differential equations.
    - Software: PyMC3, Edward, Stan (rethinking engine)...


## what makes these samplers different?

Basically the transition proposal distribution 

~ Math
P(\theta_{i+1} | \theta_i)
~


## Our model from yesterday

~~ Math
\begin{aligned} 
  y_i &\sim Normal(\mu_i, \sigma) \\
  \mu_i &= \alpha + \beta x_i  \\
  \alpha &\sim Normal(0, 20) \\
  \beta &\sim lognormal(0, 1) \\
  \sigma &\sim Exponential(1) \\
\end{aligned} 
~~

## Code in rethinking

~ Begin Columns

~ Column 
Model
~~~ Math
\begin{aligned} 
  y_i &\sim Normal(\mu_i, \sigma) \\
  \mu_i &= \alpha + \beta x_i  \\
  \alpha &\sim Normal(0, 20) \\
  \beta &\sim lognormal(0, 1) \\
  \sigma &\sim Exponential(1) \\
\end{aligned} 
~~~
~

~ Column 
Code
 ```python
 # Data
library(rethinking)
d2 <- Howell1[ Howell1$age >= 18 , ] 

# Model
ulam(alist(
  y ~ normal(mu, sigma),
  mu <- a + b * x,
  a ~ normal(0, 20),
  b ~ lognormal(0, 1),
  sigma ~ exponential(1)), 
  data = list(y = d2$height, 
              x = d2$weight),
  iter = 1000, chains = 4, cores = 4)
  ```
~ 

~ End Columns

## rethinking generates stan code

```
data{
    vector[352] y;
    vector[352] x;
}
parameters{
    real a;
    real<lower=0> b;
    real<lower=0> sigma;
}
model{
    vector[352] mu;
    sigma ~ exponential( 1 );
    b ~ lognormal( 0 , 1 );
    a ~ normal( 0 , 20 );
    for ( i in 1:352 ) {
        mu[i] = a + b * x[i];
    }
    y ~ normal( mu , sigma );
}
```

## Sampler arguments

- Chains
- Cores
- Iterations
- Warm-up (or burn-in)

## Chains and convergence

~Center
![](../figures/trace_plot.png)
~

```
> precis(fit)
        mean   sd   5.5%  94.5% n_eff Rhat4
a     112.97 2.00 109.83 116.19  1013  1.01
b       0.92 0.04   0.86   0.99  1004  1.01
sigma   5.08 0.20   4.78   5.40  1543  1.00
```

## Model checking

After fitting the model, we can use the posterior to simulate synthetic data and compare to the data used to fit the model. Discrepancies can suggest paths to improve the model.

~ Math
y_{sim} \sim P(y_{sim}|y) = P(y_{sim}|\theta) P(\theta|y)
~

## Posterior simulations

~Center
![](../figures/height_weight_posteriors_simulation.png)
~

## Posterior correlations

~Center
![](../figures/height_weight_pairs.png)
~

## Reparametrization

```python
ulam(alist(
  y ~ normal(mu, sigma),
  mu <- a + b * x,
  a ~ normal(0, 1),
  b ~ lognormal(0, 1),
  sigma ~ exponential(1)), 
  data = list(y = d2$height - mean(d2$height), 
              x = (d2$weight - mean(d2$weight))/sd(d2$weight)),
  iter = 1000, chains = 4, cores = 4)
  ```

## Posterior after centering

  ~Center
![](../figures/height_weight_pairs_centered.png)
~

## Collinear predictors

The posterior can be used to interpret weird results. 

Returning to the primate milk example:

- $K_i$: caloric content of milk in several monkey groups
- $x_i$: milk fat content
- $z_i$: milk sugar content

~~ Math
\begin{aligned}
  K_i &\sim Normal(\mu_i, \sigma) \\
  m1: \mu_i &= \alpha + \beta_x x_i \\
  m2: \mu_i &= \alpha + \beta_z z_i\\
  m3: \mu_i &= \alpha + \beta_x x_i + \beta_z z_i \\
  \alpha, \beta_x, \beta_z &\sim Normal(0, 0.5) \\
  \sigma &\sim Exponential(1)
\end{aligned}
~~

## Shifting effects

~Center
![](../figures/milk_coefs.png)
~

Smaller effects and wider intervals

## Correlated coefficients - m3 model

~Center
![](../figures/milk_coef_pairs.png)
~

## Correlated predictors

~Center
![](../figures/milk_predictor_pairs.png)
~
