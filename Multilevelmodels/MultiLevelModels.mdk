[INCLUDE=present.mdk]
Title         : Multilevel and hierarchical models
Sub Title     : Shrinkage, partial pooling, and mixed linear models
Author        : Diogo Melo
Affiliation   : Lewis-Sigler Institute of Integrative Genomics
Email         : damelo@princeton.edu
Reveal Theme  : solarized
Beamer Theme  : singapore
Package       : pstricks
Package       : pst-plot
@html .math-inline, .math-display: color=#586e75

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_SVG-full" type="text/javascript"></script>

[TITLE]

## Multilevel models 

- Multilevel models introduce structure to the parameter distribution
- Allows us to model several common processes
    - Repeated measures of the same individual
    - Block designs (some observations more similar than others)
    - Pseudo-replicates
    - Correlated observations
- Even in the absence of this type of structure, multilevel models can improve model performance 

## Baseball example 

- Batting averages
- $p$ is the probability that an attempted hit is successful.
- We can estimate the season long avg. of a player using a small sample in the beginning of the season.

ML estimates:

~ Math
\hat {p} = N_{hits}/N_{attempts}
~

## Hyper parameters

- We want to introduce some dependency between the different player's avgs. 
- This allows the information joint distribution of all players to inform the individual estimates.
- $y_i$: number of hits
- $n_i$: number of attempts

~ Math
\begin{aligned}
y_i &\sim Binomial(n_i, p_i) \\
p_i &\sim Beta(\nu_1, \nu_2) \\
\nu_1, \nu_2 &\sim lognormal(0, 1)
\end{aligned}
~

## baseball in ulam

```python
m1 = ulam(alist(
  hits ~ binomial(at_bats, avgs),
  avgs <- p[player],
  p[player] ~ beta(nu0, nu1),
  nu0 ~ lognormal(0, 1),
  nu1 ~ lognormal(0, 1)
), data = list(hits = d[,2], 
               at_bats = d[,1], 
               player = 1:N_players), 
chains = 4, cores = 4, iter = 2000)
```

## Batting averages

~ Center
![](../figures/baseball_estimates.png)
~ 

- Extreme estimates are "shrunk" towards the mean

## Shrinkage and partial pooling 

## Random intercepts model

Likelihood:

~ Math
\begin{aligned}
y_i &\sim Normal(\mu_i, sigma) \\ 
\mu_i &= \alpha_0 + \alpha_{group[i]} \\ 
\alpha_k &\sim Normal(0, \sigma_{group}) \\ 
\end{aligned}
~

Priors:
~ Math
\begin{aligned}
\alpha_0 &\sim Normal(0, 1) \\
\sigma, \sigma_{group} &\sim Half\text{-}Normal(0, 1) \\
\end{aligned}
~ 

## Blocked experimental designs

~ Math
\begin{aligned}
y_i &\sim Normal(\mu_i, sigma) \\ 
\mu_i &= \alpha_0 + \alpha_{block[i]} + \beta x_i \\ 
\alpha_k &\sim Normal(0, \sigma_{block}) \\ 
\alpha_0 &\sim Normal(0, 1) \\
\beta &\sim Normal(0, 0.3) \\
\sigma, \sigma_{block} &\sim Half\text{-}Normal(0, 1) \\
\end{aligned}
~ 

## The 8 Schools Problem

~ Center
![](../figures/8schools_paper.png){height: 600px}
~

## Random slopes model

## Replicated data

## Correlated residuals

## Phylogenetic models and Gaussian processes